{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e58499f-2503-4b6e-886a-e56eaf6a202f",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this notebook, we will preprocess the light curves to make them suitable for the model. The Light Curves can be stored as csv files in a folder with any name. However, each light curve should be its own csv with a unique name and three columns - 'mjd', 'mag', 'magerr'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee4c55f-c9cd-4c44-8f06-25570e7fe581",
   "metadata": {},
   "source": [
    "The package can be installed from pip with a simple pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f0e27-b7ab-4afe-b3f3-dd1cbcd81c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the package isn't in the enviornment that you are working in\n",
    "\n",
    "!pip install QNPy_Latte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf20bc83-0d5e-474f-8471-c77653b86020",
   "metadata": {},
   "source": [
    "### Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f8811-484d-4ff2-8af2-e9ff137e05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import QNPy_Latte.Preprocess as pr #Importing Preprocess module from the package\n",
    "from QNPy_Latte.Preprocess import transform #importing the funcion transform for transformation the data\n",
    "from QNPy_Latte.Preprocess import * #importing all e6xternal packages from Preprocess\n",
    "import shutil #Is used for creation and deletion of folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094a83a-dd96-4e63-943d-0d2efc1cb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import QNPy_Latte.SPLITTING_AND_TRAINING as st #Importing SPLITTING_AND_TRAINING module from the package\n",
    "from QNPy_Latte.SPLITTING_AND_TRAINING import * #Importing all packages from SPLITTING_AND_TRAINING module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40fec3-8f42-44db-8f78-5bee95f3f0f3",
   "metadata": {},
   "source": [
    "### Importing Data and keyword definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316048b5-6b5d-490c-8ccb-253f7b8a97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LCs = f'Light_Curves/' #The name of the folder that the Light Curves are stored in\n",
    "#Alternatively, you can have your light curves all in a folder with bands \n",
    "#SRC_LCs = f'Light_Curves/band_name/'\n",
    "file_name = f'LCs' #The suffix to attach to the new files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785d842-a6f3-4d4c-8989-6313e0acf002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Names of the files\n",
    "file_names = []\n",
    "for name in glob.glob(SRC_LCs+'/*.csv'):\n",
    "    file_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8c0a5-cdf7-46b0-a779-7181a1664723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data. This can be done in any desired manner, but the data must contain:\n",
    "#mjd - MJD or time, mag-magnitude and magerr-magnitude error. \n",
    "# In this example we used pandas python package for importing the .csv data, but numpy can be used if the data is \n",
    "#in .txt file\n",
    "#Get the data\n",
    "path = SRC_LCs\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "df_list = (pd.read_csv(file) for file in csv_files)\n",
    "data = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f679c64-5525-4859-9156-8526a24bcad3",
   "metadata": {},
   "source": [
    "### Cleaning Light Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc1317-c188-415d-912a-89de87302e33",
   "metadata": {},
   "source": [
    "We offer the option to clean the light curve by removing outlier observations. This is achieved by removing extreme outliers (magerr>1), applying a three-point median filter and removing points above a certain threshold from the 5th degree polynomial fit to the light curve. However, the threshokd is increased if too many points are removed until a maximum of 10% of points are removed (methods from Sanchez-Saez et. al. 2021 and Tachibana and Graham et al. 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75875d6-b0e7-4369-b248-e489fe16ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you would like to clean the curve\n",
    "clean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c17e5-ac2d-40b0-b563-a491f590720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean:\n",
    "    cleaned_path = f'./Cleaned_Light_Curves_{band}/'\n",
    "    clean_outliers_median(path,cleaned_path,median = True)\n",
    "    path = cleaned_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e549cf7-3ce1-478d-9a8c-aac8f3299af7",
   "metadata": {},
   "source": [
    "### Padding the Light Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c557dae0-0b08-4c4e-9a82-bf6f601c1bb6",
   "metadata": {},
   "source": [
    "We pad the light curves to ensure that they all have the same number of observations. Thus, we can batch our data for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57243019-dc57-43a9-813f-7631433bafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the light curves\n",
    "# We added the function for backward padding the curves with last observed value\n",
    "# The length for padding should remain 100 or above \n",
    "# Verbose indicates whether the confirmation of the file should be printed (>0) or nothing (=0)\n",
    "padding= pr.backward_pad_curves(path, f'./Padded_lc_{file_name}', desired_observations=100,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccd6cc-9f63-4865-b416-c6fb45db2c20",
   "metadata": {},
   "source": [
    "### Preprocessing/Transforming Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d01af-bdd0-4830-a022-671857eead55",
   "metadata": {},
   "source": [
    "We preprocess the data so that both the times and magnitudes are scaled to the range of [-2,2]. We also save the coefficients to aid in the reverse transform later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594985c6-7f26-4781-bcce-390e3ea5ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to Padded Data\n",
    "DATA_SRC = f\"./Padded_lc_{file_name}\" \n",
    "#path to folder to save preproc data (transformed data)\n",
    "DATA_DST = f\"./preproc_{file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642ba1e-da69-46e5-a636-7a90a131802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the preprocess directory\n",
    "os.makedirs(DATA_DST,exist_ok=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf20c3-ffad-41fa-92c5-a3ac994c47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the data that are going to be transformed. \n",
    "#In case that your original data is in one table, this is not needed\n",
    "files = os.listdir(DATA_SRC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7e357-a891-413e-8045-e8d0d40e85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the TR_Coeffs file\n",
    "os.makedirs('TR_Coeffs',exist_ok = True)\n",
    "trcoeff_filename = f'TR_Coeffs/trcoeff_{file_name}.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17be68a-2a19-4804-9db3-dd08dfd5fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the preprocess transformation\n",
    "number_of_points, trcoeff = pr.transform_and_save(files, DATA_SRC, DATA_DST, transform,trcoeff_file = trcoeff_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19c97d-2d4a-49d6-ba2e-9d578159440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the padded folder (Optional)\n",
    "shutil.rmtree(DATA_SRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b402b-25e6-4f9e-9af3-6b70c7b9967a",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf84301-86e6-49c9-8454-b777f9ac7841",
   "metadata": {},
   "source": [
    "We split the data into train, test, and validation folders. The split is roughly 80-10-10, but the validation folder is guarenteed to have at least two light curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643a0bf-8873-47b3-b021-8d3b07b240dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the new data source the transformed light curves\n",
    "DATA_SRC = DATA_DST #Path to transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57726cc-a6d2-421b-8450-b40d85c9b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the transformed data\n",
    "files = os.listdir(DATA_SRC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930a5f3-e53b-4dae-8c8b-8ff282005530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the folders for saving the splitted data\n",
    "st.create_split_folders(train_folder=f'./dataset_{file_name}/train/', test_folder=f'./dataset_{file_name}/test/',\\\n",
    "                        val_folder=f'./dataset_{file_name}/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1c862-fa41-47ff-95bd-8b2a713aa921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to TRAIN, TEST and VAL folders where your splitted data will be saved. \n",
    "#You can directly enter this informations in split_data function\n",
    "TRAIN_FOLDER = f'./dataset_{file_name}/train/'\n",
    "TEST_FOLDER = f'./dataset_{file_name}/test/'\n",
    "VAL_FOLDER = f'./dataset_{file_name}/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400538c-9f9a-4011-b74c-8fd384fca8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing the output folders\n",
    "# if you don't have anything in your TRAIN, TEST and VAL folders this can be scipped\n",
    "st.prepare_output_dir(TRAIN_FOLDER) \n",
    "st.prepare_output_dir(TEST_FOLDER) \n",
    "st.prepare_output_dir(VAL_FOLDER) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5f671-005a-4950-927f-063b17e3ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the function for splitting the data\n",
    "#Verbose is similar to the previous function where the confirmation should be printed or not\n",
    "st.split_data(files, DATA_SRC, TRAIN_FOLDER, TEST_FOLDER, VAL_FOLDER,verbose = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea947d9-e913-4d25-8fa7-7321abae2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the preproc folders (Optional)\n",
    "shutil.rmtree(DATA_SRC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QNPy",
   "language": "python",
   "name": "qnpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
