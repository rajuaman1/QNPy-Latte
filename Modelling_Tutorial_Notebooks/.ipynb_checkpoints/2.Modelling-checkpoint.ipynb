{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4203b15d-4fd4-431c-a8c1-ac2d373c211a",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83ae4-3cdd-43b9-a84f-7eb4b996a329",
   "metadata": {},
   "source": [
    "In this notebook, we will go through the actual modelling (the most exciting part!) of the light curves with the AttnLNP (or the latte model). There are many parameters that can be tuned, which will be defined before running the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ca5fe-7448-4532-9ccb-e340e326053c",
   "metadata": {},
   "source": [
    "### Defining Important Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61af4f72-0c59-4b3d-b817-0283b7261770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The suffix to attach to the output file\n",
    "file_name_output = 'LC'\n",
    "#The batch size for the input data\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "#Model hyperparameters\n",
    "encoding_size = 128 #Encoder MLP layer size\n",
    "latent_size = 128 #Latent dimension size\n",
    "\n",
    "attention_type = 'scaledot' #Can also use multihead, but scaledot works better\n",
    "cross_attention = True #Whether to include cross-attention in the deterministic path\n",
    "self_attention = True #Whether to include self-attention in both paths\n",
    "\n",
    "lstm_layers = 0 #The number of LSTM layers to use for pre-encoding\n",
    "lstm_size = 32 #The size of the LSTM layer\n",
    "\n",
    "use_scheduler = False # Whether to use a learning rate scheduler (has not been found to be effective)\n",
    "replace_lstm_with_gru = False # Whether to use a GRU instead of an LSTM\n",
    "bidirectional = False #Whether to use bidirectional LSTM/GRU layers\n",
    "lstm_agg = False #Whether to aggregate the latent space representations via an LSTM instead of mean pooling\n",
    "augment = True #Whether to augment the input data by randomly adding or subtracting the error on the fly\n",
    "activation = 'relu' #Can also make it 'leaky' for LeakyReLu but ReLu seems to work better\n",
    "lr = 1e-3 #The learning rate for the ADAM optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #defining the device for testing, it can be CPU or CUDA\n",
    "\n",
    "num_epochs = 1000 #The number of epochs to train\n",
    "num_runs = 1 #Amount of runs to train the model for. This runs it for another num_epochs unless it stops early\n",
    "early_stopping_limit = 500 #The number of epochs to see improvement before \n",
    "validation_epochs = 10 #Number of epochs before checking the validation data\n",
    "\n",
    "beta_tf = 0.1 #The factor of the TF in the loss\n",
    "transfer_function_length = 0 #The length of the transfer function of the data. If no transfer function, it should be 0\n",
    "tf_folder = None #Link to the folder of the transfer functions\n",
    "#tf_folder = 'Transfer_Functions/'\n",
    "#tf_folder = 'Transfer_Functions/band_name'\n",
    "\n",
    "\n",
    "param_df_path = None\n",
    "#param_df_path = 'Parameters.csv' #If there is no parameters dataframe, it should be None\n",
    "\n",
    "if param_df_path is not None:\n",
    "    param_df = pd.read_csv(param_df_path)\n",
    "    param_beta = 0.1\n",
    "    param_columns=['Log_Mass','Inclination','Log_Tau','z','Eddington_Ratio','SFinf'] #Change to the names of your columns\n",
    "    param_length = len(param_columns)\n",
    "else:\n",
    "    param_df = None\n",
    "    param_beta = 0\n",
    "    param_columns = []\n",
    "    param_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467ed02-e61d-496b-8377-58d895f885fe",
   "metadata": {},
   "source": [
    "### Importing Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd991d-9647-422f-8905-65e2e4236b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import QNPy_Latte.SPLITTING_AND_TRAINING as st #Importing SPLITTING_AND_TRAINING module from the package\n",
    "from QNPy_Latte.SPLITTING_AND_TRAINING import * #Importing all packages from SPLITTING_AND_TRAINING module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e9662-bca2-4a8b-b773-d22f56e3f248",
   "metadata": {},
   "source": [
    "### Creating the Output Folders for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6491c-126e-4350-897c-2dadc695e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'output_{file_name_output}',exist_ok=True)\n",
    "DATA_PATH_TRAIN = f\"./dataset_{file_name}/train\" #path to train folder\n",
    "DATA_PATH_VAL = f\"./dataset_{file_name}/val\" #path to val folder\n",
    "\n",
    "MODEL_PATH = f\"./output_{file_name_output}/model_{file_name_output}.pth\" #path for saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b788df-44b0-465b-ad0f-92f10f013a26",
   "metadata": {},
   "source": [
    "### Initializing model, optimizer and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e8b61-1889-4397-8432-f7532daa93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler, criterion, mseMetric, maeMetric = st.create_model_and_optimizer(device,encoding_size,latent_size,\\\n",
    "                                                                                             attention=cross_attention,self_attention=self_attention,\\\n",
    "                                                                                             use_scheduler=use_scheduler,lstm_layers = lstm_layers,\\\n",
    "                                                                                             lstm_size=lstm_size,transfer_function_length=transfer_function_length,\\\n",
    "                                                                                             parameters_length = param_length,classes = 0,\\\n",
    "                                                                                             replace_lstm_with_gru=replace_lstm_with_gru,\\\n",
    "                                                                                             bidirectional = bidirectional,activation=activation,\\\n",
    "                                                                                             lr=lr,attention_type=attention_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ef20f-2d47-46b9-aced-5d2bc2ab62f0",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222215c4-927d-4b75-adbe-15564d582cdb",
   "metadata": {},
   "source": [
    "Associated with the AttnLNP model, there are many losses. They are defined for both the train and the validation data:\n",
    "\n",
    "History_Loss - The overall loss that is used to train the model\n",
    "\n",
    "History_mse - The Mean Squared Error\n",
    "\n",
    "History_mae - The Mean Absolute Error\n",
    "\n",
    "Epoch_Counter_Loss - The epoch counter for the history loss\n",
    "\n",
    "Epoch_Counter_mse - The epoch counter for the MSE loss\n",
    "\n",
    "Epoch_Counter_mae - The epoch counter for the MAE loss\n",
    "\n",
    "History_Loss_Reconstruction - The reconstruction loss (Comes from the Gaussian LogProbLoss)\n",
    "\n",
    "History_Loss_TF - The transfer function loss (Gaussian LogProbLoss)\n",
    "\n",
    "History_Loss_param - The loss associated with the parameters (Gaussian LogProbLoss)\n",
    "\n",
    "History_KL_loss - The KL divergence between the posterior and prior latent distributions to keep the sampling coherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b08b3e-f669-48a8-9868-dea177489fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returning the trained model and all the losses associated with it\n",
    "history_loss_train,history_loss_val,history_mse_train,history_mse_val,history_mae_train,history_mae_val,\\\n",
    "epoch_counter_train_loss,epoch_counter_train_mse,epoch_counter_train_mae,epoch_counter_val_loss,epoch_counter_val_mse,epoch_counter_val_mae,\\\n",
    "history_loss_reconstruction_train,history_loss_reconstruction_val,history_loss_tf_train,history_loss_tf_val,history_loss_param_train,history_loss_param_val,\\\n",
    "history_loss_classes_train,history_loss_classes_val,history_kl_loss_train,history_kl_loss_val = st.train_model(\n",
    "    model, criterion, optimizer, scheduler, num_runs, num_epochs, early_stopping_limit, mseMetric, maeMetric, device,DATA_PATH_TRAIN,DATA_PATH_VAL,BATCH_SIZE,\\\n",
    "    beta_classifier = 0,beta_tf=beta_tf,beta_param= param_beta,tf_dir=tf_folder,param_df=param_df,param_columns=param_columns,augment = augment,validation_epochs = validation_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f46ed9-610c-4cab-b2fb-8ed7f91bbb58",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f3944-f3b1-47d3-b763-2ed941136627",
   "metadata": {},
   "outputs": [],
   "source": [
    "save=st.save_model(model, MODEL_PATH)#saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e0033-99d0-4924-b78a-6fbaafa3930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file names for saving the lists for all histories\n",
    "file_names = [f\"output_{file_name_output}/history_loss_train.csv\", f\"output_{file_name_output}/history_loss_val.csv\", f\"output_{file_name_output}/history_mse_train.csv\", f\"output_{file_name_output}/history_mse_val.csv\",\n",
    "            f\"output_{file_name_output}/history_mae_train.csv\", f\"output_{file_name_output}/history_mae_val.csv\", f\"output_{file_name_output}/epoch_counter_train_loss.csv\",\n",
    "            f\"output_{file_name_output}/epoch_counter_train_mse.csv\", f\"output_{file_name_output}/epoch_counter_train_mae.csv\", f\"output_{file_name_output}/epoch_counter_val_loss.csv\",\n",
    "            f\"output_{file_name_output}/epoch_counter_val_mse.csv\", f\"output_{file_name_output}/epoch_counter_val_mae.csv\",f\"output_{file_name_output}/history_loss_reconstruction_train.csv\", f\"output_{file_name_output}/history_loss_reconstruction_val.csv\",\\\n",
    "            f\"output_{file_name_output}/history_loss_tf_train.csv\", f\"output_{file_name_output}/history_loss_tf_val.csv\",f\"output_{file_name_output}/history_loss_param_train.csv\", f\"output_{file_name_output}/history_loss_param_val.csv\",\\\n",
    "            f\"output_{file_name_output}/history_loss_classes_train.csv\", f\"output_{file_name_output}/history_loss_classes_val.csv\",f\"output_{file_name_output}/history_kl_loss_train.csv\", f\"output_{file_name_output}/history_kl_loss_val.csv\",]\n",
    "\n",
    "# Define the lists\n",
    "lists = [history_loss_train, history_loss_val, history_mse_train, history_mse_val, history_mae_train,\n",
    "        history_mae_val, epoch_counter_train_loss, epoch_counter_train_mse, epoch_counter_train_mae,\n",
    "        epoch_counter_val_loss, epoch_counter_val_mse, epoch_counter_val_mae,history_loss_reconstruction_train,\n",
    "        history_loss_reconstruction_val,history_loss_tf_train,history_loss_tf_val,history_loss_param_train,\n",
    "        history_loss_param_val,history_loss_classes_train,history_loss_classes_val,history_kl_loss_train,history_kl_loss_val]\n",
    "\n",
    "#running the function for saving all lists with histories\n",
    "save_list= st.save_lists_to_csv(file_names, lists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QNPy",
   "language": "python",
   "name": "qnpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
